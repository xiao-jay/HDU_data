The fusion of emotional information of music audio and lyrics can effectively improve the classification performance. Multi-modal fusion methods in existing research generally include two types: feature fusion and decision fusion. In the feature fusion method, different modal feature vectors have large heterogeneous differences. If the concatenate method is used to fuse features directly, part of the emotional information will be lost in the dimensionality reduction process, resulting in reduced classification performance. In the decision fusion method, the basic linear probability fusion method only considers the output emotional probabilities of different modals, and ignores the correlation between the feature of the modals, which has certain limitations. This paper studies the significant problems of feature heterogeneity and feature correlation in multi-modal fusion, and proposes a stacking ensemble learning method for music emotion classification.
DNN can be understood as a neural network with multiple FC (fully connected layers), which can process 1D features well. The DNN in the model contains 3 hidden layers with 256, 128, and 64 nodes respectively, which are used to synthesize feature information. The input audio HSFs feature or lyrics chi-square test vector is further compressed through the DNN layer.